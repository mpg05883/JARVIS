{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53975a0",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e50a4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import wandb\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from utils.commmon import build_dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf13c09",
   "metadata": {},
   "source": [
    "Specify the dataset, model, and run configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11acf8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"multimedia\"\n",
    "temperature = 0.2\n",
    "top_p = 0.1\n",
    "api_addr = \"api.together.xyz\"\n",
    "api_port = 443\n",
    "multiworker = 1\n",
    "llm = \"meta-llama_Llama-3.3-70B-Instruct-Turbo\"\n",
    "use_demos = 2\n",
    "reformat = True\n",
    "reformat_by = \"self\"\n",
    "tag = True\n",
    "dependency_type = \"resource\"\n",
    "log_first_detail = True\n",
    "fraction = 1.0\n",
    "seed = 42\n",
    "wait_time = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913c2d4",
   "metadata": {},
   "source": [
    "Build the path to the predictions file based on the run configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "754bc9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions path: ..\\data_multimedia\\predictions_use_demos_2_reformat_by_self\\meta-llama_Llama-3.3-70B-Instruct-Turbo.json\n"
     ]
    }
   ],
   "source": [
    "data_dir = f\"data_{dataset}\"\n",
    "\n",
    "predictions_dir = build_dir_name(\n",
    "    base_name=\"predictions\",\n",
    "    use_demos=use_demos,\n",
    "    reformat_by=reformat_by,\n",
    ")\n",
    "\n",
    "predictions_path = Path(f\"../{data_dir}/{predictions_dir}/{llm}.json\")\n",
    "print(f\"Predictions path: {predictions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eeb0f1",
   "metadata": {},
   "source": [
    "Read the dataset's user requests file to count the number of user requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ac47258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of user requests: 5584\n"
     ]
    }
   ],
   "source": [
    "user_requests_path = Path(f\"../{data_dir}/user_requests.json\")\n",
    "\n",
    "num_user_requests = 0\n",
    "try:\n",
    "    with open(user_requests_path, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {user_requests_path}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"File is not valid JSON (likely JSONL format)\")\n",
    "\n",
    "print(f\"Number of user requests: {(num_user_requests := len(lines))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be36422",
   "metadata": {},
   "source": [
    "Load the predictions from the JSON file. \n",
    "- The number of predictions should be the same as the number of user requests.\n",
    "- If this isn't the case, then that means some of the API calls failed to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0d49b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff00faeee5f4462a767c665565d40e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading predictions:   0%|          | 0/5584 [00:00<?, ?prediction/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5571 predictions\n",
      "Number of predictions: 5571 does not match number of user requests: 5584\n",
      "There are 13 user requests that do not have predictions\n",
      "This is 0.23% of the user requests\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "kwargs = {\n",
    "    \"desc\": \"Loading predictions\",\n",
    "    \"unit\": \"prediction\",\n",
    "    \"total\": num_user_requests,\n",
    "}\n",
    "\n",
    "with open(predictions_path, \"r\") as f:\n",
    "    for i, line in tqdm(enumerate(f, 1), **kwargs):\n",
    "        line = line.strip()  # Remove whitespace\n",
    "        if line:  # Skip empty lines\n",
    "            try:\n",
    "                prediction = json.loads(line)\n",
    "                predictions.append(prediction)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "print(f\"Loaded {(num_predictions := len(predictions))} predictions\")\n",
    "\n",
    "if num_predictions != num_user_requests:\n",
    "    print(\n",
    "        f\"Number of predictions: {num_predictions} does not match number \"\n",
    "        f\"of user requests: {num_user_requests}\"\n",
    "    )\n",
    "    difference = num_user_requests - num_predictions\n",
    "    percentage = difference / num_user_requests * 100\n",
    "    print(f\"There are {difference} user requests that do not have predictions\")\n",
    "    print(f\"This is {percentage:.2f}% of the user requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b4122",
   "metadata": {},
   "source": [
    "Initialize [W&B](https://wandb.ai/mpgee-usc/Eval%20Project?nw=nwusermpgee). configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ff7f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    \"dataset\": dataset,\n",
    "    \"temperature\": temperature,\n",
    "    \"top_p\": top_p,\n",
    "    \"api_addr\": api_addr,\n",
    "    \"api_port\": api_port,\n",
    "    \"multiworker\": multiworker,\n",
    "    \"llm\": llm,\n",
    "    \"use_demos\": use_demos,\n",
    "    \"reformat\": reformat,\n",
    "    \"reformat_by\": reformat_by,\n",
    "    \"tag\": tag,\n",
    "    \"dependency_type\": dependency_type,\n",
    "    \"log_first_detail\": log_first_detail,\n",
    "    \"fraction\": fraction,\n",
    "    \"seed\": seed,\n",
    "    \"wait_time\": wait_time,\n",
    "}\n",
    "\n",
    "init_kwargs = {\n",
    "    \"name\": f\"upload_{dataset}_{llm}_artifacts\",\n",
    "    \"tags\": [\n",
    "        f\"dataset={dataset}\",\n",
    "        f\"llm={llm}\",\n",
    "    ],\n",
    "    \"group\": \"sae_stats\",\n",
    "    \"notes\": f\"Uploading artifacts for {dataset} with {llm}\",\n",
    "    \"job_type\": \"artifact_upload\",\n",
    "    \"mode\": \"online\",\n",
    "}\n",
    "\n",
    "predictions_kwargs = {\n",
    "    \"name\": f\"{llm}_{dataset}_predictions\",\n",
    "    \"type\": \"predictions\",\n",
    "    \"description\": f\"{llm} predictions for TaskBench's {dataset.capitalize()} dataset\",\n",
    "    \"metadata\": run_config,\n",
    "}\n",
    "\n",
    "\n",
    "metrics_kwargs = {\n",
    "    \"name\": f\"{llm}_{dataset}_metrics\",\n",
    "    \"type\": \"metrics\",\n",
    "    \"description\": f\"{llm} evaluation metrics for TaskBench's {dataset.capitalize()} dataset\",\n",
    "    \"metadata\": run_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a15e5d",
   "metadata": {},
   "source": [
    "Upload the predictions to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d05bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmpgee\u001b[0m (\u001b[33mmpgee-usc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb\\wandb\\run-20250823_153742-0ac94tin</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mpgee-usc/Eval%20Project/runs/0ac94tin' target=\"_blank\">upload_multimedia_meta-llama_Llama-3.3-70B-Instruct-Turbo_artifacts</a></strong> to <a href='https://wandb.ai/mpgee-usc/Eval%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mpgee-usc/Eval%20Project' target=\"_blank\">https://wandb.ai/mpgee-usc/Eval%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mpgee-usc/Eval%20Project/runs/0ac94tin' target=\"_blank\">https://wandb.ai/mpgee-usc/Eval%20Project/runs/0ac94tin</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upload_multimedia_meta-llama_Llama-3.3-70B-Instruct-Turbo_artifacts</strong> at: <a href='https://wandb.ai/mpgee-usc/Eval%20Project/runs/0ac94tin' target=\"_blank\">https://wandb.ai/mpgee-usc/Eval%20Project/runs/0ac94tin</a><br> View project at: <a href='https://wandb.ai/mpgee-usc/Eval%20Project' target=\"_blank\">https://wandb.ai/mpgee-usc/Eval%20Project</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb\\wandb\\run-20250823_153742-0ac94tin\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "run = wandb.init(**init_kwargs, config=run_config)\n",
    "\n",
    "predictions_artifact = wandb.Artifact(**predictions_kwargs)\n",
    "metrics_artifact = wandb.Artifact(**metrics_kwargs)\n",
    "\n",
    "metrics_dir = build_dir_name(\n",
    "    base_name=\"metrics\",\n",
    "    use_demos=use_demos,\n",
    "    reformat_by=reformat_by,\n",
    ")\n",
    "\n",
    "metrics_path = Path(f\"../{data_dir}/{metrics_dir}/{llm}.json\")\n",
    "\n",
    "predictions_artifact.add_file(predictions_path)\n",
    "metrics_artifact.add_file(metrics_path)\n",
    "\n",
    "run.log_artifact(predictions_artifact)\n",
    "run.log_artifact(metrics_artifact)\n",
    "\n",
    "# TODO: Change this so it logs the overall metrics to the summary\n",
    "\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
